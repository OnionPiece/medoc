# To understand OpenShift

## 前言

本篇文档的目的是帮助首次接触容器云平台的用户，即需要在容器云平台做应用开发，测试，以及进行应用运维的技术人员，能够较快速的了解什么是OpenShift，怎么用等相关问题，以便于能在容器云平台上快速的上手。如不加说明，则容器云平台或容器平台均指OpenShift。


## 容器 vs 虚机

首先，在进一步展开前，这里浅谈一些容器与虚机的对比，希望能在感性上让大家理解我们为什么需要容器。之所以不对比物理服务器，因为虚机在某种程度上不过是将物理服务器的资源通过虚拟化技术在计算、存储、网络方面进行了分割；从下往上看，它们确实有所不同，但是从上往下，对于应用而言，是不会关心底层环境到底是什么的。

既然说到应用其实不关系底层到底是什么，那么为什么还要纠结容器与虚机呢？这关乎到人，关乎到开发，测试，运维。传统来说，开发一款应用，你需要多个环境，对应多组服务器，分别用于研发，测试，以及正式上线，甚至更多。这就至少带来两个问题，其一，环境的一致性怎么维护，比如常见的“在我的环境上跑的好好的，一到你的环境就出问题，那肯定是你环境的问题”；其二，这带来了资源的浪费。IaaS不会解决第一个问题，也无法缓解第二个问题（你申领的虚机资源可能比你的应用实际需求的资源要多），毕竟IaaS的目标是便利性。此外，更不要忽略了虚机的维护成本。

那么，容器会能否解决这两个问题呢，答案是肯定的。容器的本质是提供一个充分必要的运行时环境，而你要做的只是把可运行的代码放在里面，通过指定服务的启动命令及参数，让容器引擎帮你把程序跑起来。这就意味着，当你使用的运行环境确定后，更多的你只需要关注你的代码。 **而这个运行时环境，就叫做镜像。** 于是乎，研发、测试、运维可以拿相同的镜像来跑相同代码，环境的一致性得以保证。此外，镜像一般都是通过精心裁剪的，只包含了相应程序能够跑起来的基本依赖，因此也为精致的资源供给提供了基础。

## Docker, Kubernetes and OpenShift

既然选择使用容器，那么用使用docker, Kubernetes, OpenShift又有什么区别呢？

  - 首先docker是个容器引擎，它可以管理容器，包括启停、网络、挂卷等等，但它是个单点的，即只负责一个节点上的容器，只有一个单点的容器管理程序是很难满足需求的；
  - 于是就有了Kubernetes(以下简称K8S，这也是开源社区的叫法），它负责面向集群的容器的管理，但它的核心是调度，而至于各个节点间的容器网络，镜像仓库，存储挂卷，监控预警等，它并没有直接提供方案，而是提供了相应的接口，便于其他厂商对接或集成。所以如果使用K8S，你就需要自己负责对容器网络，镜像仓库，存储挂卷，监控预警等进行选型、配置、维护；
  - 而OpenShift呢，它在K8S的基础上，将上述“外围”需求进行了集成和封装，所以如果使用了OpenShift，那些东西都会为你准备好，并且在此基础上，维持了插件可替换的原则，可以将网络、存储、监控等换成你钟意的组件。

一个通俗但不难么恰当的例子，docker是个引擎，K8S在docker的基础上做完了整个动力系统，而OpenShift则是装好了的一辆车。

或者参照OpenStack，那么docker就是qemu，而K8S是nova，而OpenShift基本上就是OpenStack了，所以说OpenShift才是实际意义上的平台。

## 应用上云的基本流程

### 构建镜像

首先，我们需要根据项目，从互联网（一般是docker hub）上选择合适的基础镜像，例如选择用java镜像或者php镜像。选择后，需要将我们自己的代码糅合到镜像里。这里最基础的做法是通过编写一个叫做Dockerfile的文件，然后通过docker命令，构造出一个以基础镜像为底，包含应用运行内容的新镜像。

如何编写Dockerfile并不在本文范围内，略过不提。

### 上传镜像

构建完后，就需要将新的镜像上传/推送到容器云平台上的镜像仓库。

容器平台之所以需要一个镜像仓库，有点像公司内部会需要搭建软件仓库一样，但不同在于，镜像仓库的消费者是平台本身。容器平台为了维护用户的业务，当一个容器挂掉后，会需要重新拉取镜像并运行容器，以“弥补空缺”。而这个拉取镜像的地方就是镜像仓库。

### 部署准备

完成了镜像推送后，就可以考虑开始在容器平台上逐步通过容器部署出应用了。但在正式开始前，我们需要再谈一下跑在容器里的程序。

首先跑在容器内的程序被限定只能跑在前台(foreground)，以此，所有程序输出到标准输出的内容都将被认为是容器的日志。

其次，程序跑在前台后，容器与程序"同生共死"。这就意味着如果程序需要重启来刷新配置，启动参数等，则必然会导致当前容器挂掉。容器能否再次被拉起来，则取决与它的重启策略。常见的重启策略是永远尝试重启，即除非容器被删除，否则一直按照容器最近一次的配置，重启容器。因此，为了能让对程序的配置有效、生效，我们需要先将相关的配置写到容器的配置中。

同理，环境变量也有相同的问题。运行时环境的变量在容器启动时就已经根据镜像的定义被定义好了，而后续所有的修改都无法覆盖原有的环境变量，因此对环境变量修改的办法只有先将想要设置的变量写入到容器的配置中，之后再重新部署容器。

然后，我们还需要考虑一下存储。容器在启动后，系统会在特定的本地盘路径下，分配一个存储空间给容器使用，容器产生的所有落盘数据都将写入到这个存储空间。但这个存储空间是临时的，也就是说，当容器一旦死掉后，该存储空间就会被释放，因此已经落盘的数据将会消失。为此，我们需要利用持久化存储，产生持久化卷，挂载到容器内部的某个目录中，来为容器中的程序提供持久化的落盘。

最后，我们需要理解一下的是，由于容器的本质是一个满足程序运行的充分必要的运行时环境，因此它本身并不具备相关机制来保证程序稳定健康的运行，所以才会有容器与程序同生共死的关系。而容器平台的价值就在于，提供配套机制，来保证用户的应用不会因为容器的消亡而不可用，来保证用户的业务不会因为容器的生死而不稳定。

### 部署容器

在理解了部署容器需要明确的基本问题后，我们就可以正式部署容器了。

容器平台中管理容器的最小单元叫做Pod。Pod中可以定义及创建多个容器，这些容器将共享相同的网络和本地存储。例如你可以把一对前后端容器放在同一个Pod里，这样便于他们的进程间通信，但却也带来了维护成本。因此，通常一个Pod里只放一个容器，也因此，可以将Pod说成是容器。

容器的部署可以通过创建一个Pod来完成，但是通常不会这么做，因为这样只能部署出一个容器，而实际使用中，出于高可用或者为了应对高发的业务流量的目的，通常我们会为同一个服务创建多个容器副本，这样的多副本的容器可以称为容器组。

为此容器平台提供了 Replication Controllers，即副本控制器。使用了副本控制器部署出来的容器，平台会监控容器的副本数量（即使是1），会保证在用户容器实际在跑的数量与用户定义的副本数一致。举例来说，如果你的应用中的一个服务容器的副本数为2，那么当有一个容器故障后，副本控制器会自动按照容器之前的配置重新拉起一个新的容器，来满足2的副本数要求。

但即使有了副本控制器，也还是不够的，因为伴随研发的迭代，必然会有新镜像的产生。那么已有的应用，就需要面临升级。你可以选择蓝绿发布，A/B发布，或者金丝雀升级。在这里先讲金丝雀升级。

容器平台提供了Deployments以及Deployment Configs（简称dc）的资源对象来提供对容器部署的进一步管控（含盖了副本控制器的功能），这些增加的管控中就包含了金丝雀升级。在升级时，新版本的容器会逐个被创建，同时伴随包含副本控制器在内的机制的参与，旧版本的容器会被逐个销毁。在这个过程中，每次创建出新的容器后，相关机制会对该容器进行就绪检查，如果检查通过了，就认为该容器部署成功；否则就会持续检查直到超时，当超时发生时，就会认为升级失败，整个升级都会回滚。也因此， **在OpenShift中，金丝雀升级也被成为滚动升级** 。

综上所述，对于用户容器，通常都是通过dc来进行创建以及管理的。

### 创建服务

在部署完容器（包括环境变量、挂载持久化卷），以及搞定副本和升级后，接下来就要考虑如何将容器提供的服务暴露出来了。这里的暴露包含两部分，一种是最直观的，暴露到外面，让最终用户访问；而另一种是暴露在平台内部，让用户的多个服务之间可以相互调用。这里先讲后一种。

通常来说，一款应用可能有多个组件、服务组成，而进行开发时，又不可能将若干个服务放在同一个容器里，因此就会形成多个容器组来构成同一个应用。为此，服务，或者说不同容器组之间的相互调用不可或缺。

而在容器之间相互调用，最朴素的办法就是通过容器的IP，但这存在至少三个问题:
  - 怎么知道对方的IP；
  - 对方的IP“变了”怎么办；
  - 以及当对方包含多个副本时如何做到调用请求的负载均摊。

在容器平台上，容器的IP是在容器部署时随机分配的，并且也是伴随容器的销毁而回收的，可以说是不可预测，不可预定，不可保留的。并且容器的运行异常、升级、扩缩容，以及底层节点维护带来的容器被疏散等情况，都会导致容器的朝生暮死。在这种情况下，通过容器IP来串联服务，会显得费力且不易于维护。

为此，K8S提供了Service资源对象来解决服务串联的问题。当我们创建Service时，K8S会利用内部的机制，将Service与一组满足匹配条件的容器进行关联，形成一个逻辑上的前端。这种关联会被相应的机制进行监听，以保证当容器发生生死变迁时，从Service总能到对应的容器组。并且Service在创建时将会被分配一个IP，而这个IP相较于容器的IP更为稳定，因为只有人为删除Service时，这个IP才会被回收。在创建了Service之后，服务之间可以利用彼此Service IP及端口进行调用。同时Service还带有了一定的负载均衡能力，能将收到的流量按照轮训或者源IP的方式分发给后端的容器。

当然Service不止提供了IP，还有域名。毕竟只有IP也是不够的，因为Service的IP也只能在被创建后才能知道，并且也不便于服务替换。Service域名的默认格式为::

    YourServiceName.YourNamespaceName.svc.cluster.local

当你使用如上格式的域名去访问域名时，容器平台内置的DNS服务会将域名解析成对的Service IP。

由Service将Pod进行“封装”，是容器平台的基本套路。

### 暴露服务

在创建完了Service，完成了对容器组的组织以及暴露内部调用支持后，接下来就该考虑如何将容器服务暴露到外面，让最终客户访问了。
容器平台提供了两种暴露方式，基于HTTP/HTTPS的方式，和基于TCP的方式。前者更多的用于互联网应用，而后者可用于公司内网服务或应用的横向打通。

但无论哪种方式，它们的处理机理是相同的：

  - 或借助域名的方式（这需要平台所在的物理集群的上游DNS将相应的域名指向集群），将外部请求打到平台所在物理集群的节点上；
  - 或通过NAT将外部的TCP请求进行转化，或通过代理将外部HTTP(S)请求进行转化，转化为平台内部能接受的虚拟网络流量；
  - 将转化后的流量，按照容器网络间的访问处理。
  
对于TCP方式暴露的服务，只需要修改Service的，就可以完成暴露。而HTTP(S)方式暴露的服务，则需要创建OpenShift的Route资源对象。

Route对象在创建时，需要指定一个Service进行关联，借此暴露到外部的域名才有能力找到对应的容器。当然，同Route也支持其他一些可选参数进行配置:

  - 自定义的域名，如果用户不指定，那么平台将会在泛域名的基础上，根据相应规则产生域名；
  - 额外的Service以及对应的权重，这是实现蓝绿、A/B发布的基础；
  >无论蓝绿还是A/B，都需要先创建两组容器及Service，然后添加到同一个Route。不同在于，对于A/B，两个Service权重不会为0；而对于蓝绿，添加时蓝版的权重为0，而服务切换后，绿版的权重为0。
  - 访问路径，默认是根，例如你可以指定"/images"来修改对应服务能够被匹配到的URL；当你有多个应用需要借助同一个公网注册域名来提供服务时，可以选择在这里进行配置，而不是自己部署nginx服务来处理流量分发；
  - 是否为secure，即HTTPS方式，已经选择HTTPS终结方式，上传证书等。

### 小结

本节在介绍应用上云的基础流程之上，扩展的讲了关于容器，容器平台的相关知识，这些包括:

  - 容器的环境变量、配置、持久化卷
  - 容器的副本控制、金丝雀升级
  - 容器服务的相互间调用，与容器平台内建域名
  - 容器的TCP暴露，HTTP(S)暴露，与蓝绿、A/B发布

当理解并掌握了以上知识，就具备了在容器平台上创建、部署应用的基础。当然实际中，我们可能会需要考虑更多的问题，后续将进一步展开。

## 容器平台的更多功能

### 智能构建与增量构建

在应用上云的基础流程中，我们提到了可以通过编写Dockerfile，然后运行docker命令的方式来构建镜像，这种构建方式被称为push构建。当然，也可以将Dokerfile添加到你的应用的项目中，例如添加到github上你的项目下，这时就可以使用容器平台提供的dockerfile构建功能，并且平台在构建完了后会自动将新镜像推送到镜像仓库中，这种方式被称为Dockerfile构建。

之所以没有在前面提及，因为无论在本地还是在平台上，构建的本质是一样的，你都需自己编写Dockerfile，而且本地更利于调试。而在平台构建则可以考虑在本地连接平台网络不稳定或网速慢时使用。

此外，容器平台还提供了一种构建方式，即S2I构建，或者说叫智能构建。S2I是Source to Image的意思，即从源码到镜像，支持基于编译型或解释型语言的项目，但是需要基础镜像满足S2I的要求。

无论是Dockerfile构建，还是S2I构建，都可以在容器平台上获取相应的web hook，来配置在基于git的代码管理平台上(如github、gitlab)，这样，当新的代码推送或合并后，就可以通过web hook触发容器平台上新版本代码的镜像构建。

但S2I构建与Dockerfile又有所不同，通过S2I构建出来的镜像，可以进行增量构建，即新一轮的构建可以在上一次构建出来的镜像的基础上进行。这样做的价值在于加速了构建过程。因为通常构建时，在基础镜像内，除了拉取代码，还需要安装各种依赖，以及进行编译。而增量构建则在依赖和编译两个步骤上，存在复用的可能，借此，整个构建过程减少了不必要依赖下载安装过程，以及编译过程。

### 容器的健康检查

前文中，在金丝雀升级的部分，我们提到过，在升级过程中，平台会对容器进行就绪检查，如果容器长时间无法就绪直到超时，就会认为升级失败。而这里的就绪检查，就属于一种健康检查。

容器平台提供了两种健康检查机制，readiness probe，即就绪(探针)检查，与liveness probe，即存活(探针)检查。当然，就绪检查也不仅仅在金丝雀升级中被使用，一旦这两个检查被配置到容器，它们会在容器的生命周期里一直周期性的进行检查。因为对容器平台而言，它无法判断容器内的程序什么时候会ready，而什么时候突然无法相应请求，什么时候容器内的程序还可以提供服务，而什么时候突然宕掉。

无论那种检查，我们都可以选择三种检查方式之一进行配置，即在容器内执行命令的方式，从外部探测容器的TCP端口的方式，以及从外部以某个路径探测容器的HTTP服务的方式。

配置后，就绪检查将为容器能否接受流量提供依据，即非就绪的容器无法接受流量，即使容器已经启动了。这是合理的，毕竟容器启动，并不意味着程序已经就绪，它可能还处于初始化阶段。而存活检查将为容器是否需要重启提供依据，毕竟容器健康，并不意味着容器内的程序健康，一些非致命的错误或异常可以维持容器的运行，但是却会导致程序无法再提供服务。

### 敏感数据管理

容器平台提供的配置管理功能的价值在于使得配置可以和镜像，和项目代码解耦。这个思路可以套用到敏感数据，例如密码，秘钥等。

不同于配置文件与环境变量，敏感数据在上传到平台，是不应该被查看的。敏感数据可以在平台上使用，例如向服务提供密码去连接数据库，向构建功能提供秘钥去拉取gitlab的代码等。但同时，具有使用权限的人员应该负责确保容器内，或者利用程序的相关接口，无法查询到所引用的敏感数据的实际内容。因为平台是无法对敏感数据在用户容器内如何使用做担保的。

### 容器组的扩缩容

在介绍容器部署时，我们提到了副本控制器，容器平台通过副本控制器来保证用户实际运行的容器数量与预设的副本数一致。

而关于副本数，除了在创建/部署容器时可以指定外，还可以在容器部署后，手动修改，或者利用自动伸缩机制来处理。

当副本数为0时，用户相关服务已有的容器都将会被销毁，不再重新创建，但是保留了创建时所有的配置。之后，可以在任何时候恢复副本数，使容器能重新跑起来，使得用户的服务可用。

相对的，当副本数很大时，平台不会完全保证期望的副本数量一定能被创建出来，这取决与用户的配额，以及平台的资源量。

对于自动伸缩(autoscale)，容器平台当前只支持以CPU作为伸缩的标准。在利用dc对象部署出容器组后，用户可以为dc对象添加自动伸缩，并指明最少和最多的副本数，以及容器组的CPU扩容阈值。并且应该与就绪检查配合使用。