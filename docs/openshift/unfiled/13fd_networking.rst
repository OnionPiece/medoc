********************
13FD项目网络问题回顾
********************


计算节点没有默认路由
====================

部署环境中的节点并不完全在同一网段，因此当计算节点没有默认路由时会导致origin-node服务无法启动，因为`ip r get 8.8.8.8`对主网卡的探测失效，从而导致网络插件起不来。修复这个问题，有两种方法:

  1. 添加默认路由，推荐这样做，对于大多数网络拓扑情况的适应能力强;
  2. 在node-config.yaml中配置nodeIP，对于某些情况，例如完全隔离的私有网络，这是一种选择。

对于第二种方式，需要明确的是，无论如何都需要保证计算节点是可以和所有的Master节点通信的；否则，在当前架构下，这意味着从管理面和数据面，计算节点可能会因为同网段Master节点的失联而导致自己和集群失联。


集群外负载均衡节点无法掌握集群内部的连通性
==========================================

首次发现这个故障是在使用keepalived对某些服务进行高可用支持时，因为操作失误导致了Master节点的路由被覆盖了，Master和计算节点通信时使用的是keepalived配置的VIP，而不是已经注册的SDN IP。这就造成了该Master节点与所有计算节点在SDN网络上的不可达，因此当客户访问部署在计算节点上的CaaS Portal时，无法访问。

外部负载均衡所选择的算法是源IP，所以有的客户会遇到这个问题，而部分客户则没有。

解决当时的故障的方法，就是修复故障Master节点的路由。但是更进一步地，想要解决当外部负载均衡知道某个Master无法访问CaaS Portal时“踢掉”该Master的问题，则会引出一些列目前无解的问题。

首先，这是一个探测的问题。区别于常见的健康检查，那种情况只探测一层，即LB到Server，并且不考虑后面的Server其实也是一个Server。而对于两层LB，即外层LB -- LB -- Server的情况，我们需要外层LB能够探测到Server是否可达。毫无疑问，想要达到这个目的，靠tcp check是不行的，因此就需要外部LB配置http check，靠http ckeck穿透内部LB到达Server。https://github.com/OnionPiece/netns-topo/tree/master/topologies/haproxy_1to3_vertical_lb.yaml 作为一个例子给出了这样的展示。

但这就带来一个问题，目前外部LB的配置是没有做acl的，所有访问80端口的流量都将走到同一组backend。那么就意味着，如果一个Master到CaaS Portal的探测失败，它将同时不能代理其他用户容器的业务流量，即使它只是到CaaS Portal节点的链路有问题，这就带来了资源使用上的不合理的弃用。但反过来，如果CaaS Portal发生故障，那么所有的Master都将无法代理用户容器业务流量，意味着整个集群业务网的瘫痪。

为此，我们需要考虑第二个问题，是否要业务管理分backend。这会带来一定的性能损耗，但是幸运的是，外部LB如果由我们搭建，那么这两个节点将只用于做负载均衡，计算资源应该是OK的。问题在于，分backend后，可以针对管理，即针对CaaS Portal做探测，那么对于业务呢，是否也要做一些探测。毫无疑问的是，目前我们无法对所有的用户容器服务做探测，毕竟外部LB在集群之外，不在OpenShift管辖范围内，其次，这会进一步消耗外部LB的计算资源。当然想做的话，可以自己写个服务，监听OpenShift的Routes，来更新外部LB的配置，但这样的价值大吗？

最后，也不存在折中思路，即通过探测Master节点到各个计算节点的SDN可达性做健康检查，来提供给外部LB做针对业务流量http check。

所以，但从网络的角度想解决这个问题，目前来看是不太可能的。但至少，我们可以有的改进点:

  - 在外部LB上对管理和业务进行分backend，毕竟如果平台管理页面都登录不了，对于用户来说是很难接受的；
  - 需要借助监控等手段，当发现集群内节点的可达性出现问题时，能够及时报警。这个相对好做一些，例如可以在Master和计算节点上分别跑haproxy，将其他节点的tun0 IP加入到backend的server，然后另一个程序周期性地通过socket进行检查，只要出现L4TOUT就可以认为SDN链路出现了问题。
